import sys
sys.path.insert(0, './yolov5')
# from ByteTrack.yolox.tracker.byte_tracker import BYTETracker


# from yolov5.Detect import Detect
from yolov5.models.common import DetectMultiBackend
from yolov5.utils.downloads import attempt_download
from yolov5.utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams
from yolov5.utils.general import (LOGGER, Profile, check_file, check_img_size, check_imshow, check_requirements,
                                  colorstr, increment_path, non_max_suppression, print_args, scale_boxes,
                                  strip_optimizer)
from yolov5.utils.torch_utils import select_device, smart_inference_mode



import matplotlib.pyplot as plt
import supervision as sv
import time, datetime
from pathlib import Path
import cv2,os,torch,threading
import torch.backends.cudnn as cudnn
from random import randint
import requests, json, base64

import numpy as np
from datetime import datetime


import UVI.SecondLevelFunc.countingv2 as countingv2
import UVI.SecondLevelFunc.get_current_config as get_current_config
import UVI.SecondLevelFunc.WriteImgAndTxt as WriteImgAndTxt
import UVI.FirstLevelFunc.load_config as load_config


configs = load_config.load_config()

uploadNameMap = configs['uploadNameMap']
stop_run_flag = configs['stop_run_flag']
FPS = configs['FPS']
UP_LINE = configs['up_line']
DOWN_LINE = configs['down_line']
TARGET_WIDTH = configs['target_width']
TARGET_HEIGHT = configs['target_height']
current_save_dir = configs['current_project_dir']

COUNT_NUMBER = configs['COUNT_NUMBER']

SOURCE = configs['source_points']

# è®¡ç®— LINE_start (å– [0] å’Œ [3] ç‚¹çš„ä¸­ç‚¹)
LINE_start = [(SOURCE[0][0] + SOURCE[3][0]) / 2, 
              (SOURCE[0][1] + SOURCE[3][1]) / 2]

# è®¡ç®— LINE_end (å– [1] å’Œ [2] ç‚¹çš„ä¸­ç‚¹)
LINE_end = [(SOURCE[1][0] + SOURCE[2][0]) / 2, 
            (SOURCE[1][1] + SOURCE[2][1]) / 2]


start, end = sv.Point(x=0, y=int(LINE_start[1])), sv.Point(x=1920, y=int(LINE_end[1]))
TARGET = np.array(
    [
        [0, 0],
        [TARGET_WIDTH - 1, 0],
        [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],
        [0, TARGET_HEIGHT - 1],
    ]
)

global valid_car_info
valid_car_info = []

class ViewTransformer:
    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:
        # source = source.astype(np.float32)
        # target = target.astype(np.float32)
        # ç¡®ä¿ source å’Œ target æ˜¯ NumPy æ•°ç»„
        source = np.array(source, dtype=np.float32)
        target = np.array(target, dtype=np.float32)
        self.m = cv2.getPerspectiveTransform(source, target)

    def transform_points(self, points: np.ndarray) -> np.ndarray:
        if points.size == 0:
            return points

        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)
        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)
        return transformed_points.reshape(-1, 2)

def exit_program():
    print("é€€å‡ºç¨‹åºï¼Œç»ˆæ­¢çº¿ç¨‹")
    os._exit(0)  # ç«‹å³å¼ºåˆ¶é€€å‡ºæ•´ä¸ªPythonç¨‹åºï¼ˆåŒ…æ‹¬æ‰€æœ‰çº¿ç¨‹ï¼‰


# å¯ç”¨æ™ºèƒ½æ¨ç†æ¨¡å¼
@smart_inference_mode()
def run(weights=configs['current_weights'],  # model path or triton URL
        source=configs['current_source'],  # file/dir/URL/glob/screen/0(webcam)
        data='yolov5/data/coco128.yaml',  # dataset.yaml path
        imgsz=[500],  # inference size (height, width)q
        conf_thres=0.25,  # confidence threshold
        iou_thres=0.45,  # NMS IOU threshold
        max_det=1000,  # maximum detections per image
        device=0,  # cuda device, i.e. 0 or 0,1,2,3 or cpu
        view_img=False,  # show results
        save_txt=False,  # save results to *.txt
        save=True,  # save images/videos
        ref_time='2023-01-01 00:00:00',  # '%Y-%m-%d %H:%M:%S'
        classes=None,  # filter by class: --class 0, or --class 0 2 3
        agnostic_nms=False,  # class-agnostic NMS
        augment=False,  # augmented inference
        visualize=False,  # visualize features
        update=False,  # update all models
        project='runs/detect',  # save results to project/name
        name='exp',  # save results to project/name
        exist_ok=False,  # existing project/name ok, do not increment
        half=False,  # use FP16 half-precision inference
        dnn=False,  # use OpenCV DNN for ONNX inference
        vid_stride=1,  # video frame-rate stride
        camera_device='192.168.10.3',
        ):
    
    lost_frame_count = 0  # åˆå§‹åŒ–ä¸¢å¸§è®¡æ•°å™¨
    MAX_LOST_FRAMES = 30  # è¿ç»­ä¸¢å¸§30æ¬¡åˆ™é‡å¯ï¼Œå¤§çº¦30ç§’
    global stop_run_flag
    
    
    # åˆå§‹åŒ– ByteTrack
    bytetrack= sv.ByteTrack(track_activation_threshold=0.25,lost_track_buffer=FPS,minimum_matching_threshold=0.8,frame_rate=FPS,minimum_consecutive_frames=1)
    
    # åˆå§‹åŒ–ä¸Šä¼ çš„å­—å…¸ï¼ŒåŒ…å«è®¾å¤‡ç¼–ç ã€å›¾åƒç­‰ä¿¡æ¯
    uploadBaseDic = {"deviceCode": '28254', "image": '0', "vehicleModel": '0', "speed": 0, "lanesNumber": 0,
                     "detectionTime": '0'}
    uploadBaseDic['deviceCode'] = camera_device

    # åˆå§‹åŒ–å·¥å…·ç±» ToolVehicle

    print("source",source)
    print("weights",weights)
    print("camera_device",camera_device)

    #æµ‹é€Ÿ
    view_transformer = ViewTransformer(source=SOURCE, target=TARGET)
    

    # picture_num = ToolVehicle(total_num)  # ç»Ÿè®¡æ€»æ•°
    source = str(source)  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç±»å‹
    save_img = save and (not source.endswith('.txt'))  # åˆ¤æ–­æ˜¯å¦ä¿å­˜æ¨ç†åçš„å›¾åƒ
    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)  # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶
    is_url = source.lower().startswith(('rtsp://', 'rtmp://', 'http://', 'https://'))  # åˆ¤æ–­æ˜¯å¦ä¸ºURL
    # webcam = source.isnumeric() or source.endswith('.txt') or (is_url and not is_file)
    webcam = source.isnumeric() or source.endswith('.txt') or is_url  # æ£€æŸ¥æ˜¯å¦ä¸ºæ‘„åƒå¤´æˆ–æµåª’ä½“
    screenshot = source.lower().startswith('screen')  # æ£€æŸ¥æ˜¯å¦ä¸ºæˆªå›¾

    # if is_url and is_file:
    #    source = check_file(source)  # download

    # httpå¼€å¤´, .mp4ç»“å°¾ä¸ºå½•æ’­ï¼Œ  httpå¼€å¤´, .flvç»“å°¾ä¸ºç›´æ’­
    # playback = (source.startswith('http') and source.endswith('.mp4'))  # æ•°æ®æ¥æº
    playback = (source.startswith('http') and source.endswith('.flv'))  # åˆ¤æ–­æ˜¯å¦ä¸ºç›´æ’­æµ

    # åˆ›å»ºä¿å­˜ç»“æœçš„ç›®å½•
    if save_img:
        save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # è‡ªåŠ¨é€’å¢è·¯å¾„
        (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
        number_path = str(save_dir / 'number.txt')  # è®¡æ•°ä¿¡æ¯ä¿å­˜ä½ç½®

    # åŠ è½½æ¨¡å‹
    device = select_device(device)  # é€‰æ‹©è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰
    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)  # åˆå§‹åŒ–YOLOæ¨¡å‹
    stride, names, pt = model.stride, model.names, model.pt  # è·å–æ¨¡å‹æ­¥é•¿ã€ç±»åˆ«åç§°å’Œæ¨¡å‹ç±»å‹
    imgsz = check_img_size(imgsz, s=stride)  # æ£€æŸ¥å›¾åƒå¤§å°æ˜¯å¦ç¬¦åˆæ¨¡å‹è¦æ±‚
    print("è¾“å‡ºæ£€æµ‹ç±»åˆ«ï¼š", names)  # æ‰“å°æ£€æµ‹åˆ°çš„ç±»åˆ«

    # Dataloaderæ•°æ®åŠ è½½å™¨ï¼Œç”¨äºåŠ è½½è§†é¢‘æˆ–å›¾åƒæµ
    bs = 1  # batch_size
    if webcam:  # å¦‚æœæ˜¯æ‘„åƒå¤´æˆ–æµåª’ä½“
        cudnn.benchmark = True  # è®¾ç½®ä¸ºTrueåŠ é€Ÿå›ºå®šå›¾åƒå¤§å°çš„æ¨ç†
        check_imshow(warn=True)  # æ£€æŸ¥æ˜¯å¦æ”¯æŒimshow
        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)  # åŠ è½½æµåª’ä½“æ•°æ®
        bs = len(dataset)  # æ›´æ–°batch_size
    elif screenshot:  # å¦‚æœæ˜¯æˆªå›¾
        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)  # åŠ è½½æˆªå›¾æ•°æ®
    else:  # åŠ è½½æœ¬åœ°è§†é¢‘æˆ–å›¾åƒ
        cudnn.benchmark = True  # è®¾ç½®ä¸ºTrueåŠ é€Ÿå›ºå®šå›¾åƒå¤§å°çš„æ¨ç†
        check_imshow(warn=True)  # æ£€æŸ¥æ˜¯å¦æ”¯æŒimshow
        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)  # åŠ è½½å›¾åƒæ•°æ®

    vid_path, vid_writer = [None] * bs, [None] * bs  # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨

    # åˆå§‹åŒ–ä¸Šä¼ æ•°æ®
    total_count = 0
    down_detail = {"Motorcycle": 0, "Car": 0, "Bus": 0, "Tractor": 0, "L_truck": 0, "XL_truck": 0, "XXL_truck": 0,
                   "XXXL_truck": 0, "Container car": 0, "Electric vehicle": 0, "Total": 0}
    up_detail = {"Motorcycle": 0, "Car": 0, "Bus": 0, "Tractor": 0, "L_truck": 0, "XL_truck": 0, "XXL_truck": 0,
                 "XXXL_truck": 0, "Container car": 0, "Electric vehicle": 0, "Total": 0}
    trackInfoList = []  # è·Ÿè¸ªä¿¡æ¯åˆ—è¡¨

    headers = {'Content-Type': 'application/json'}

    # httpå½•æ’­çŠ¶æ€æ—¶ï¼Œç»™å‰ç«¯å‘é€â€œå¼€å§‹åˆ†æâ€ä¿¡å·
    if playback:
        upload_address = upload_addr3 + '/' + source.split('/')[-1] + '/' + 'InProcess'
        r = requests.get(upload_addr3, headers=headers, params=json.dumps({'fileName': source.split('/')[-1], 'status': 'InProcess'}))

        # è·å¾—å›æ”¾è§†é¢‘çš„å†å²å¼€å§‹æ—¶é—´æˆ³ï¼Œç”¨äºå åŠ åˆ†ææ—¶é—´
        ref_time = datetime.strptime(ref_time, '%Y-%m-%d %H:%M:%S')
        ref_time = datetime.timestamp(ref_time)

    # Run inferenceï¼ŒåŠ é€Ÿæ¨ç†é€Ÿåº¦
    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup
    seen, deeosortNum, windows, dt = 0, 0, [], (Profile(), Profile(), Profile(), Profile())  # åˆå§‹åŒ–è®¡æ—¶å™¨
    t0 = time.time()  # å¼€å§‹è®¡æ—¶
    DataNum, CarTotal = 0, 0
    old_time = datetime.now().strftime('%Y-%m-%d')
    # labels = defaultdict(lambda:np.array([0]))

    # æ³¨æ„ï¼šè¯·åœ¨å¾ªç¯å¼€å§‹å‰å®šä¹‰è¿™ä¸ªæ–‡ä»¶ï¼ˆåªå†™ä¸€æ¬¡ï¼‰

    open("track.txt", "w").close()  # æ¸…ç©ºæ–‡ä»¶ï¼ˆåªåœ¨ç¬¬ä¸€å¸§ï¼‰

    # éå†æ¯ä¸€å¸§å›¾åƒï¼Œæ‰§è¡Œæ¨ç†å’Œè·Ÿè¸ª
    for frame_idx, (path, im, im0s, vid_cap, timestamp, ret_flag) in enumerate(dataset):


        # print(type(path), type(im), type(im0s))
        #
        # print("é¢„å¤„ç†åçš„å›¾åƒ im shape: ", im[0])
        # print("åŸå§‹å›¾åƒ im0s shape: ", im0s[0])
        # print("è·¯å¾„ path: ", path)
        # print("é¢„å¤„ç†åçš„å›¾åƒ vid_cap: ", vid_cap)
        # print("åŸå§‹å›¾åƒ timestamp: ", timestamp)
        # print("è·¯å¾„ ret_flag: ", ret_flag)
        # break  # æŸ¥çœ‹ä¸€å¸§åé€€å‡º


        # print(f"[DEBUG] æ­£åœ¨å¤„ç†å¸§ {frame_idx}")
        # æ–°å¢é€€å‡ºæ ‡å¿—æ£€æŸ¥
        if stop_run_flag:
            print("æ”¶åˆ°ä¸­æ­¢æ ‡å¿—ï¼Œrun() æ­£åœ¨é€€å‡º...")
            break
        # print("è£å‰ªåçš„å›¾åƒshape:", img.shape)
        # ---------------------è®¾ç½®åŒºåŸŸæ£€æµ‹çš„èŒƒå›´  å¦‚æœä¸éœ€è¦æ˜¾ç¤ºï¼Œå¯ä»¥åœ¨ä¸‹é¢æ³¨é‡Šæ‰--------------------------
        # Detect.region_detect(im, webcam)
        # ---------------------------------------------------------------------------------------------------------------
        if not ret_flag:
            lost_frame_count += 1
            LOGGER.warning(f"ç¬¬ {lost_frame_count} æ¬¡æœªæˆåŠŸè¯»å–å¸§")
            if lost_frame_count >= MAX_LOST_FRAMES:
                LOGGER.error("è¿ç»­ä¸¢å¸§è¾¾åˆ°ä¸Šé™ï¼Œå‡†å¤‡é€€å‡ºç¨‹åºï¼")
                stop_run_flag = True  # æ ‡è®°é€€å‡º
                exit_program()
                break
            time.sleep(1)
            continue
        else:
            lost_frame_count = 0  # æˆåŠŸè¯»å–å¸§ï¼Œé‡ç½®è®¡æ•°å™¨

        # img = im.copy()  # æ·±æ‹·è´
        # img = img.astype(np.float32)
        #
        #
        # im_np = np.transpose(img, (1, 2, 0))  # (C, H, W) â†’ (H, W, C)


        # å¤„ç†å›¾åƒæ•°æ®ï¼Œè½¬æ¢ä¸ºæ¨¡å‹å¯ç”¨çš„æ ¼å¼
        with dt[0]:
            # im=EqualizeHistRGB(im,model) # CUPå¢å¼º
            # im = torchvision_histogram_equalization(im,model) # GPUå¢å¼º
            im = torch.from_numpy(im).to(model.device)  # å°†numpyæ•°ç»„è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼Œå¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            im = im.half() if model.fp16 else im.float()  # æ ¹æ®æ¨¡å‹æ˜¯å¦ä½¿ç”¨FP16ï¼Œé€‰æ‹©åŠç²¾åº¦æˆ–æµ®ç‚¹ç²¾åº¦
            im /= 255  # å°†åƒç´ å€¼0 - 255å½’ä¸€åŒ–åˆ°0.0 - 1.0ä¹‹é—´
            if len(im.shape) == 3:
                im = im[None]  # æ‰©å±•ç»´åº¦ï¼Œé€‚åº”æ‰¹å¤„ç†
        s = ""  # åˆå§‹åŒ–ä¸ºä¸€ä¸ªç©ºå­—ç¬¦ä¸²
        # æ¨ç†é˜¶æ®µï¼Œä½¿ç”¨YOLOv5æ¨¡å‹è¿›è¡Œç›®æ ‡æ£€æµ‹
        with dt[1]:
            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False  # å¤„ç†å¯è§†åŒ–è·¯å¾„
            pred = model(im, augment=augment, visualize=visualize)  # æ‰§è¡Œæ¨ç†ï¼Œè·å¾—é¢„æµ‹ç»“æœ

        # éæå¤§å€¼æŠ‘åˆ¶ï¼Œå»é™¤å¤šä½™çš„æ£€æµ‹æ¡†
        with dt[2]:
            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)

        # Second-stage classifier (optional)
        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)

        # å¤„ç†é¢„æµ‹ç»“æœ
        with dt[3]:
            # éå†æ¯å¼ å›¾ç‰‡çš„é¢„æµ‹ç»“æœ
            for i, det in enumerate(pred):
                seen += 1  # å¢åŠ å¤„ç†å¸§çš„è®¡æ•°

                if webcam:  # å¦‚æœæ˜¯æ‘„åƒå¤´è¾“å…¥ï¼Œé€å¸§å¤„ç†
                    p, im0, frame = path[i], im0s[i].copy(), dataset.count
                    # s += f'{i}: '
                    # ---------------------ç”»å‡ºåŒºåŸŸæ£€æµ‹çš„èŒƒå›´  å¦‚æœä¸éœ€è¦æ˜¾ç¤ºï¼Œå¯ä»¥åœ¨ä¸‹é¢æ³¨é‡Šæ‰--------------------------
                    # Detect.show_region_detection(im0)
                    # ---------------------------------------------------------------------------------------------------
                else:  # å¦‚æœæ˜¯æ–‡ä»¶æˆ–è§†é¢‘æµ
                    p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)
                    # p, im0, frame = path, cv2.resize(im0s.copy(),(1280,720)), getattr(dataset, 'frame', 0)
                    # ---------------------ç”»å‡ºåŒºåŸŸæ£€æµ‹çš„èŒƒå›´  å¦‚æœä¸éœ€è¦æ˜¾ç¤ºï¼Œå¯ä»¥åœ¨ä¸‹é¢æ³¨é‡Šæ‰--------------------------
                    # æ˜¾ç¤ºå¤„ç†åçš„å›¾åƒ
                    # Detect.show_region_detection(im0)
                    # ---------------------------------------------------------------------------------------------------
                p = Path(p)  # å°†è·¯å¾„è½¬æ¢ä¸ºPathå¯¹è±¡
                save_path = str(save_dir / p.name)  # ä¿å­˜å›¾åƒçš„è·¯å¾„
                if len(det):  # å¦‚æœæ£€æµ‹åˆ°ç›®æ ‡
                    # print(frame_idx)
                    # å°†æ£€æµ‹æ¡†çš„åæ ‡ä»å›¾åƒå°ºå¯¸ç¼©æ”¾åˆ°åŸå›¾å¤§å°

                    det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()
                    CarTotal += len(det)








                    # x1, y1, x2, y2 = map(int, det[0, :4].tolist())
                    # #
                    # # # 2ï¸âƒ£ ç”»æ¡†ï¼ˆç»¿è‰²ï¼Œçº¿å®½ 2ï¼‰
                    # cv2.rectangle(im0, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)
                    # #
                    # # # 3ï¸âƒ£ æ˜¾ç¤ºå›¾åƒï¼ˆä½¿ç”¨ OpenCV çš„çª—å£ï¼‰
                    # cv2.imshow("Image with Bounding Box", im0)
                    # cv2.waitKey(0)  # ç­‰å¾…é”®ç›˜è¾“å…¥
                    # cv2.destroyAllWindows()





                    det_cpu = det.cpu()
                    xyxy = det_cpu[:, :4].numpy()  # è½¬æˆnumpy ndarray
                    confidence = det_cpu[:, 4].numpy()
                    class_id = det_cpu[:, 5].int().numpy()  # å¦‚æœæ•´æ•°ç±»åˆ«ä¹Ÿè½¬
                    dets = sv.Detections(
                        xyxy = xyxy,
                        confidence = confidence,
                        class_id = class_id
                    )
                    dets = bytetrack.update_with_detections(dets)

                    # dets = reversed(det.cpu())
                    # dets = sv.Detections.from_yolov5(dets)
                    # dets = bytetrack.update_with_detections(dets)
                    # # dets:åæ ‡ï¼Œç±»åˆ«ç½®ä¿¡åº¦ï¼Œç±»åˆ«idï¼Œè¯†åˆ«æ¡†id





                    # æ¯ä¸€å¸§å¤„ç†åï¼Œä¿å­˜ç»“æœ
                    with open("track.txt", "a") as f:
                        for det in dets:  # results æ˜¯ bytetrack.update_with_detections(dets) è¿”å›çš„
                            tid = det[4]
                            x1, y1, x2, y2 = det[0]  # åæ ‡ä¸º xyxy æ ¼å¼
                            score = det[2]
                            class_id = det[3]

                            # è½¬ä¸º tlwhï¼ˆå·¦ä¸Šè§’ + å®½é«˜ï¼‰
                            x = x1
                            y = y1
                            w = x2 - x1
                            h = y2 - y1

                            # frame_id +1 æ˜¯ä¸ºäº†ä» 1 å¼€å§‹è®¡æ•°ï¼ˆç¬¦åˆ MOT æ ¼å¼ï¼‰
                            line = f"{frame_idx + 1},{tid},{x:.2f},{y:.2f},{w:.2f},{h:.2f},{score:.2f},{class_id},-1,-1\n"
                            # line = f"{i + 1},{tid},{x:.2f},{y:.2f},{w:.2f},{h:.2f},{score:.2f},-1,-1,-1\n"
                            f.write(line)




                    deeosortNum += 1
                    # å¦‚æœæœ‰æ£€æµ‹ç»“æœï¼Œç»Ÿè®¡è½¦æµé‡å¹¶ä¼°è®¡é€Ÿåº¦
                    if len(dets) > 0:
                        # ç»Ÿè®¡è½¦æµé‡å¹¶ä¼°è®¡é€Ÿåº¦
                        # crossed_in, crossed_out = line_zone.trigger(dets)
                        # total_count, down_detail, up_detail, valid_car_info = tool_vehicle.counting(dets, names)  # ç»Ÿè®¡è½¦æµé‡
                        global COUNT_NUMBER
                        im0, valid_car_info, COUNT_NUMBER = countingv2.countingv2(count=COUNT_NUMBER, dets=dets, cls_names=names, view_transformer=view_transformer, FPS=FPS, max_y=TARGET_HEIGHT, up_line=UP_LINE, down_line=DOWN_LINE, is_draw=False, im0=im0)
                        # å¦‚æœæœ‰æœ‰æ•ˆçš„è½¦è¾†ä¿¡æ¯ï¼Œ å‘é€è½¦æµé‡è®¡æ•°åŠæµ‹é€Ÿç»“æœ
                        if len(valid_car_info) > 0:
                            upload_dic = uploadBaseDic
                            # å¦‚æœæ˜¯å›æ”¾æ¨¡å¼ï¼Œè®¡ç®—å†å²æ—¶é—´
                            if playback:
                                # time_consume = (int)(frame_idx * (1/25))   #ç”¨äºè®¡ç®—å›æ”¾è§†é¢‘çš„å†å²å½“å‰æ—¶é—´ï¼Œå•ä½ï¼šç§’
                                time_consume = (int)(timestamp / 1000)  # æ—¶é—´æˆ³è½¬åŒ–ä¸ºç§’
                                history_time = datetime.fromtimestamp(time_consume + ref_time)  # è®¡ç®—å†å²æ—¶é—´
                                upload_dic['detectionTime'] = history_time.strftime('%Y-%m-%d %H:%M:%S')  # æ ¼å¼åŒ–æ—¶é—´
                            else:
                                upload_dic['detectionTime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')  # å½“å‰æ—¶é—´

                            # éå†æ¯ä¸ªè½¦è¾†ï¼Œä¸Šä¼ å›¾ç‰‡å’Œè½¦è¾†ä¿¡æ¯
                            for pass_car in valid_car_info:
                                cropImg = im0[pass_car[1]:pass_car[3], pass_car[0]:pass_car[2]]  # è£å‰ªå‡ºè½¦è¾†çš„å›¾åƒåŒºåŸŸ
                                __, buffer = cv2.imencode('.jpg', cropImg)  # å°†è£å‰ªå›¾åƒç¼–ç ä¸ºJPEGæ ¼å¼
                                base64_cropImg = base64.b64encode(buffer.tobytes()).decode('utf-8')  # ç¼–ç ä¸ºBase64æ ¼å¼
                                upload_dic['image'] = base64_cropImg  # è®¾ç½®ä¸Šä¼ çš„å›¾åƒ
                                upload_dic['vehicleModel'] = uploadNameMap[names[int(pass_car[4])]]  # è®¾ç½®è½¦è¾†ç±»å‹
                                upload_dic['speed'] = pass_car[5]  # è®¾ç½®è½¦é€Ÿ
                                upload_dic['lanesNumber'] = pass_car[6]  # è®¾ç½®è½¦é“å·

                                # æ„å»ºæ–‡ä»¶è·¯å¾„å’Œæ–‡ä»¶å
                                now_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
                                date_time = datetime.now().strftime('%Y-%m-%d')

                                # åˆ›å»ºä¸€ä¸ªä¿å­˜å›¾ç‰‡ç›®å½•ï¼Œä»¥å½“å‰æ—¥æœŸå‘½å                                     # todo
                                date_path_img = os.path.join(f'{current_save_dir}/{date_time}/img')
                                if not os.path.exists(date_path_img):
                                    os.makedirs(date_path_img)

                                if old_time == date_time:
                                    DataNum = DataNum + 1
                                else:
                                    old_time = date_time
                                    DataNum = 0

                                date_path_txt = os.path.join(f'{current_save_dir}/{date_time}/txt')
                                if not os.path.exists(date_path_txt):
                                    os.makedirs(date_path_txt)

                                now_time = str(now_time) + '_' + str(DataNum) + "_" + str(randint(0, 100))

                                # ä¿å­˜å›¾ç‰‡                                                            # todo
                                img_name = f'{now_time}_img.jpg'
                                img_path = os.path.join(date_path_img, img_name)

                                txt_name = f'{now_time}_info.txt'
                                txt_path = os.path.join(date_path_txt, txt_name)

                                TaskWrite = threading.Thread(name='WriteData', target=WriteImgAndTxt.WriteImgAndTxt, kwargs={"Im": im0, "img_path": img_path, "txt_path": txt_path, "upload_dic": upload_dic, "txt_name": txt_name, "current_folder": date_path_txt, "device_code1": configs['device_code1']})  # å†™æ–‡ä»¶åˆ°æ–‡ä»¶å¤¹
                                TaskWrite.start()

                        # åœ¨UIä¸­ç”»å‡ºæ£€æµ‹æ¡†åŠä¼°è®¡é€Ÿåº¦å€¼

                        # tool_vehicle.draw_boxes_speed(im0, outputs, names)
                        # for c in valid_car_info:
                        #     l=f"#{names[int(c[4])]}:{int(c[5])}km/h"
                        #     t_size = cv2.getTextSize(l, cv2.FONT_HERSHEY_PLAIN, 2, 2)[0]
                        #     x1=int(c[0])
                        #     y1=int(c[1])
                        #     x2=int(c[2])
                        #     y2=int(c[3])

                        #     cv2.rectangle(im0, (x1, y1), (x2, y2), [0,0, 255], 3)  # ç”»å‡ºè½¦å‹çš„é¢„æµ‹æ¡†
                        #     cv2.line(im0,(812, 189),(1075, 188),[0,0,0],3)
                        #     cv2.line(im0,(1075, 188),(968, 684),[0,0,0],3)
                        #     cv2.line(im0,(968, 684),(43, 676),[0,0,0],3)
                        #     cv2.line(im0,(43, 676),(812, 189),[0,0,0],3)
                        #     cv2.circle(im0, (x2, y2), radius=4, color=(0, 0, 255), thickness=5)  # å°†é¢„æµ‹æ¡†å³ä¸‹è§’æ ‡å‡ºæ¥
                        #     cv2.rectangle(im0, (x1, y1), (x1 + t_size[0] + 3, y1 + t_size[1] + 4), [0,0, 255], -1)  # ç”»å‡ºæ ‡ç­¾çš„èƒŒæ™¯æ¡†
                        #     cv2.putText(im0, l, (x1, y1 + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 2, [255, 255, 255], 2)  # å†™å‡ºæ ‡ç­¾
                # if True:

                # video_savev2(vid_cap=vid_cap,im0=im0)
                # TaskVideo = threading.Thread(name='Video',target=video_savev2, kwargs={"im0":im0,"vid_cap":vid_cap,"save_path":save_path})  # å†™æ–‡ä»¶åˆ°æ–‡ä»¶å¤¹
                # TaskVideo.start()
                # ,"vid_cap":vid_cap,"vid_path":vid_path,
                # video_save(im0,vid_cap,vid_path,save_path,vid_writer)

                # ä¿å­˜å¸¦æœ‰æ£€æµ‹ç»“æœçš„å›¾åƒæˆ–è§†é¢‘
        # è®°å½•æ¨ç†æ—¶é—´
        # LOGGER.info( f"{'' if len(det) else '(no detections),'}{(dt[0].dt)*1E3:.1f}ms,{(dt[1].dt)*1E3:.1f}ms,{(dt[2].dt)*1E3:.1f}ms,{(dt[3].dt)* 1E3:.1f}ms")  # å•æ¬¡å…¨éƒ¨æ—¶é—´
        # LOGGER.info(f"{s}{'' if len(det) else '(no detections), '}{dt[1].dt * 1E3:.1f}ms") # Inference Only

    # Print results

    tsum = tuple(x.t * 1E3 for x in dt)  # total speeds


    
    # DSTime=tsum[3]/deeosortNum
    DSTime = tsum[3] / deeosortNum if deeosortNum != 0 else 0
    t = tuple(x.t / seen * 1E3 for x in dt)  # speeds per image
    LOGGER.info(f'åœ¨æ€»å…±{seen}å¸§å›¾åƒä¸­æ£€æµ‹åˆ°äº†{CarTotal}è¾†è½¦,è·Ÿè¸ªå¤„ç†äº†{deeosortNum}å¸§å›¾åƒå¹¶è¯†åˆ«åˆ°äº†{DataNum}è¾†æœ‰æ•ˆè½¦è¾†')
    LOGGER.info(f'Speed: {t[0]:.1f} ms pre-process, {t[1]:.1f}ms inference, {t[2]:.1f}ms NMS, {DSTime:.1f} ms DeepSort')
    
    LOGGER.info('Done. (%.3fs)' % (time.time() - t0))
    cv2.destroyAllWindows()  # å…³é—­çª—å£
    if save_img:
        s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if save_txt else ''
    # LOGGER.info(f"Results saved to {colorstr('bold', save_dir)}{s}")
    # å¦‚æœé€‰æ‹©æ›´æ–°æ¨¡å‹ï¼Œå»é™¤ä¼˜åŒ–å™¨ä»¥å‡å°‘æ¨¡å‹å¤§å°
    if update:
        strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)

    # run() ç»“æŸå¤„æ·»åŠ 
    stop_run_flag = True
    cv2.destroyAllWindows()
    LOGGER.info("ğŸ“½ï¸ run() è‡ªç„¶ç»“æŸï¼Œè®¾ç½® stop_run_flag = True ä»¥è§¦å‘é‡å¯")
    exit_program()
